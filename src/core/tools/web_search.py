from typing import Optional, List, Dict, Any
from langchain_core.tools import tool
from pydantic import BaseModel, Field
from loguru import logger
from ddgs import DDGS
import httpx
from bs4 import BeautifulSoup
import re

from src.config.settings import settings

class SearchInput(BaseModel):
    query: str = Field(..., description="è¦æœç´¢çš„å…³é”®è¯æˆ–é—®é¢˜")
    max_results: int = Field(5, description="è¿”å›çš„æœ€å¤§ç»“æœæ•°é‡ (é»˜è®¤ä¸º 5)")
    region: str = Field("wt-wt", description="æœç´¢åœ°åŒºä»£ç  (ä¾‹å¦‚ 'wt-wt' è¡¨ç¤ºå…¨çƒ, 'cn-zh' è¡¨ç¤ºä¸­å›½)")

class ExploitSearchInput(BaseModel):
    query: str = Field(..., description="æ¼æ´åç§°æˆ– CVE ç¼–å· (ä¾‹å¦‚ 'ThinkPHP 5 RCE', 'CVE-2019-11043')")
    max_results: int = Field(5, description="è¿”å›çš„æœ€å¤§ç»“æœæ•°é‡ (é»˜è®¤ä¸º 5)")

class WebContentInput(BaseModel):
    url: str = Field(..., description="è¦è·å–å†…å®¹çš„ç½‘é¡µ URL")

@tool(args_schema=SearchInput)
async def web_search(query: str, max_results: int = 5, region: str = "wt-wt") -> str:
    """
    ä½¿ç”¨ DuckDuckGo è¿›è¡Œç½‘ç»œæœç´¢ã€‚
    é€‚ç”¨äºè·å–å®æ—¶ä¿¡æ¯ã€æŸ¥æ‰¾æŠ€æœ¯æ–‡æ¡£ã€è§£å†³ç¼–ç¨‹é—®é¢˜æˆ–è·å–æœ€æ–°æ–°é—»ã€‚
    """
    return await _perform_search(query, max_results, region)

@tool(args_schema=ExploitSearchInput)
async def search_exploits(query: str, max_results: int = 5) -> str:
    """
    ä¸“æ³¨äºæœç´¢å®‰å…¨æ¼æ´ POC å’Œ Exploit ä»£ç ã€‚
    ä¼˜å…ˆä½¿ç”¨ GitHub API æœç´¢ï¼Œå¦‚æœå¤±è´¥åˆ™å›é€€åˆ°é€šç”¨æœç´¢ã€‚
    """
    logger.info(f"æ‰§è¡Œ Exploit æœç´¢: {query}")
    
    # 1. ä¼˜å…ˆå°è¯• GitHub API æœç´¢ (æ›´ç¨³å®šï¼Œé’ˆå¯¹ä»£ç )
    try:
        github_results = await _search_github(query, max_results)
        if github_results:
            return github_results
    except Exception as e:
        logger.warning(f"GitHub æœç´¢å¤±è´¥: {e}")
    
    # 2. å¦‚æœ GitHub æ²¡æ‰¾åˆ°æˆ–å¤±è´¥ï¼Œå›é€€åˆ° DuckDuckGo
    logger.info("GitHub æœç´¢æ— ç»“æœæˆ–å¤±è´¥ï¼Œå›é€€åˆ°é€šç”¨æœç´¢...")
    dork_query = f"{query} POC exploit github"
    return await _perform_search(dork_query, max_results, region="wt-wt")

async def _search_github(query: str, max_results: int) -> Optional[str]:
    """
    ä½¿ç”¨ GitHub Search API æœç´¢ä»£ç ä»“åº“
    """
    api_url = "https://api.github.com/search/repositories"
    # æ·»åŠ  'poc' æˆ– 'exploit' å…³é”®è¯ä»¥æé«˜å‡†ç¡®æ€§
    search_q = f"{query} poc OR {query} exploit"
    params = {
        "q": search_q,
        "sort": "stars",
        "order": "desc",
        "per_page": max_results
    }
    
    async with httpx.AsyncClient(verify=False, proxy=settings.SCAN_PROXY, timeout=10.0) as client:
        resp = await client.get(api_url, params=params)
        
        if resp.status_code == 200:
            data = resp.json()
            items = data.get("items", [])
            if not items:
                return None
                
            formatted = f"ğŸ™ GitHub æœç´¢ç»“æœ ('{search_q}'):\n\n"
            for i, item in enumerate(items, 1):
                name = item.get("full_name")
                url = item.get("html_url")
                desc = item.get("description") or "No description"
                stars = item.get("stargazers_count")
                updated = item.get("updated_at")
                
                formatted += f"{i}. {name} (â­ {stars})\n"
                formatted += f"   ğŸ”— {url}\n"
                formatted += f"   ğŸ“ {desc}\n"
                formatted += f"   ğŸ•’ Updated: {updated}\n\n"
            return formatted
            
        elif resp.status_code == 403:
            logger.warning("GitHub API Rate Limit Exceeded")
            return None
        else:
            logger.warning(f"GitHub API Error: {resp.status_code}")
            return None

@tool(args_schema=WebContentInput)
async def fetch_web_content(url: str) -> str:
    """
    è·å–æŒ‡å®š URL çš„ç½‘é¡µå†…å®¹ï¼ˆæ–‡æœ¬å’Œä»£ç ï¼‰ã€‚
    é€‚ç”¨äºè¯»å– GitHub ä»£ç æ–‡ä»¶ã€æŠ€æœ¯åšå®¢æ–‡ç« æˆ–æ¼æ´è¯¦æƒ…é¡µã€‚
    """
    logger.info(f"æ­£åœ¨è·å–ç½‘é¡µå†…å®¹: {url}")
    try:
        # å¤„ç† GitHub Blob URL -> Raw URL
        # e.g. https://github.com/user/repo/blob/main/file.py -> https://raw.githubusercontent.com/user/repo/main/file.py
        if "github.com" in url and "/blob/" in url:
            raw_url = url.replace("github.com", "raw.githubusercontent.com").replace("/blob/", "/")
            logger.info(f"æ£€æµ‹åˆ° GitHub Blob URLï¼Œè½¬æ¢ä¸º Raw URL: {raw_url}")
            url = raw_url

        # ç¬¬ä¸€æ¬¡å°è¯•ï¼šä½¿ç”¨ä»£ç†
        try:
            async with httpx.AsyncClient(verify=False, proxy=settings.SCAN_PROXY, follow_redirects=True, timeout=20.0) as client:
                response = await client.get(url)
                response.raise_for_status()
                return _parse_response(response, url)
        except Exception as e:
            logger.warning(f"ä½¿ç”¨ä»£ç†è·å–ç½‘é¡µå†…å®¹å¤±è´¥: {e}")
            if settings.SCAN_PROXY:
                logger.info("å°è¯•ç›´æ¥è¿æ¥ (ä¸ä½¿ç”¨ä»£ç†)...")
                # ç¬¬äºŒæ¬¡å°è¯•ï¼šç›´è¿
                async with httpx.AsyncClient(verify=False, follow_redirects=True, timeout=20.0) as client:
                    response = await client.get(url)
                    response.raise_for_status()
                    return _parse_response(response, url)
            else:
                raise e

    except Exception as e:
        logger.error(f"è·å–ç½‘é¡µå†…å®¹å¤±è´¥: {e}")
        return f"è·å–ç½‘é¡µå†…å®¹å¤±è´¥: {e}"

def _parse_response(response: httpx.Response, url: str) -> str:
    content_type = response.headers.get("content-type", "")
    
    # å¦‚æœæ˜¯çº¯æ–‡æœ¬æˆ–ä»£ç  (GitHub Raw)
    if "text/plain" in content_type or "application/json" in content_type or "raw.githubusercontent.com" in url:
        return f"ğŸ“„ åŸå§‹å†…å®¹ ({url}):\n\n{response.text[:10000]}" # é™åˆ¶è¿”å›é•¿åº¦

    # å¦‚æœæ˜¯ HTMLï¼Œæå–æ­£æ–‡
    soup = BeautifulSoup(response.text, "html.parser")
    
    # ç§»é™¤è„šæœ¬å’Œæ ·å¼
    for script in soup(["script", "style", "nav", "footer", "header"]):
        script.decompose()
        
    # æå–æ–‡æœ¬
    text = soup.get_text(separator="\n")
    
    # æ¸…ç†ç©ºè¡Œ
    lines = (line.strip() for line in text.splitlines())
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    clean_text = '\n'.join(chunk for chunk in chunks if chunk)
    
    return f"ğŸ“„ ç½‘é¡µå†…å®¹ ({url}):\n\n{clean_text[:8000]}..." # é™åˆ¶è¿”å›é•¿åº¦

async def _perform_search(query: str, max_results: int, region: str) -> str:
    logger.info(f"æ­£åœ¨æ‰§è¡Œç½‘ç»œæœç´¢: {query} (max={max_results}, region={region})")
    try:
        results = []
        # ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„ä»£ç†ï¼Œæˆ–è€… settings é…ç½®
        # å¦‚æœè¿æ¥å¤±è´¥ï¼Œå¯ä»¥å°è¯•ä¸ä½¿ç”¨ä»£ç†
        proxy = settings.SCAN_PROXY
        
        # ç¬¬ä¸€æ¬¡å°è¯•
        try:
            with DDGS(proxy=proxy, timeout=20, verify=False) as ddgs:
                # ddgs 9.10.0+ requires positional 'query' not 'keywords' for text()
                # å…¼å®¹æ€§å¤„ç†
                try:
                    ddgs_gen = ddgs.text(query, region=region, max_results=max_results)
                except TypeError:
                    # Fallback for older versions
                    ddgs_gen = ddgs.text(keywords=query, region=region, max_results=max_results)
                    
                for r in ddgs_gen:
                    results.append(r)
        except Exception as e:
            logger.warning(f"ç¬¬ä¸€æ¬¡æœç´¢å°è¯•å¤±è´¥ (proxy={proxy}): {e}")
            # å¦‚æœé…ç½®äº†ä»£ç†ä½†å¤±è´¥äº†ï¼Œå°è¯•ç›´è¿ (å¯èƒ½ä»£ç†ä¸ç¨³å®š)
            if proxy:
                logger.info("å°è¯•ç›´æ¥è¿æ¥ (ä¸ä½¿ç”¨ä»£ç†)...")
                with DDGS(timeout=20, verify=False) as ddgs:
                    try:
                        ddgs_gen = ddgs.text(query, region=region, max_results=max_results)
                    except TypeError:
                        ddgs_gen = ddgs.text(keywords=query, region=region, max_results=max_results)
                        
                    for r in ddgs_gen:
                        results.append(r)
            else:
                raise e
        
        if not results:
            return "æœªæ‰¾åˆ°ç›¸å…³ç»“æœã€‚"
            
        formatted_results = f"ğŸ” æœç´¢ç»“æœ ('{query}'):\n\n"
        for i, res in enumerate(results, 1):
            title = res.get('title', 'No Title')
            link = res.get('href', '#')
            snippet = res.get('body', 'No description available.')
            
            formatted_results += f"{i}. {title}\n"
            formatted_results += f"   ğŸ”— {link}\n"
            formatted_results += f"   ğŸ“ {snippet}\n\n"
            
        return formatted_results.strip()

    except Exception as e:
        error_msg = f"æœç´¢å¤±è´¥: {str(e)}"
        logger.error(error_msg)
        return error_msg

import httpx
from bs4 import BeautifulSoup
from langchain_core.tools import tool
from pydantic import BaseModel, Field
from loguru import logger
import asyncio
from typing import List, Optional

class PocLibrarySearchSchema(BaseModel):
    keyword: str = Field(..., description="æœç´¢å…³é”®è¯ï¼Œä¾‹å¦‚ 'æ³›å¾®', 'Spring', 'ThinkPHP'")
    max_results: int = Field(10, description="æœ€å¤§è¿”å›çš„ POC æ•°é‡ï¼Œé»˜è®¤ä¸º 10")

@tool(args_schema=PocLibrarySearchSchema)
async def search_poc_library(keyword: str, max_results: int = 10) -> str:
    """
    ä»æ¼æ´æƒ…æŠ¥åº“ (biu.life) æœç´¢å¹¶æå–åŒ…å«å®æˆ˜ POC ä»£ç çš„æ¼æ´è¯¦æƒ…ã€‚
    
    ç‰¹ç‚¹:
    1. ä»…ç­›é€‰å¸¦æœ‰ "POC" æ ‡ç­¾çš„é«˜ä»·å€¼æ¼æ´ã€‚
    2. è‡ªåŠ¨è¿›å…¥è¯¦æƒ…é¡µæŠ“å–å…·ä½“çš„ POC/Exp ä»£ç ç‰‡æ®µã€‚
    3. è¿”å›æ ¼å¼åŒ–çš„æ¼æ´æè¿°å’Œä»£ç ï¼Œå¯ç›´æ¥ç”¨äºæ¼æ´éªŒè¯ã€‚
    """
    base_url = "https://rss.biu.life"
    search_url = f"{base_url}/ti/search"
    
    logger.info(f"æ­£åœ¨ POC åº“ä¸­æœç´¢: {keyword}")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    results = []
    
    try:
        async with httpx.AsyncClient(verify=False, timeout=30.0) as client:
            # 1. æœç´¢åˆ—è¡¨é¡µ
            try:
                resp = await client.get(search_url, params={"q": keyword}, headers=headers)
                resp.raise_for_status()
            except Exception as e:
                return f"æœç´¢è¯·æ±‚å¤±è´¥: {str(e)}"

            soup = BeautifulSoup(resp.text, 'html.parser')
            items = soup.find_all('li', class_='poc-item')
            
            logger.info(f"æ‰¾åˆ° {len(items)} ä¸ªç›¸å…³æ¡ç›®ï¼Œæ­£åœ¨ç­›é€‰ POC...")
            
            tasks = []
            valid_items = []

            # 2. ç­›é€‰å¸¦æœ‰ POC æ ‡ç­¾çš„æ¡ç›®
            for item in items:
                if len(valid_items) >= max_results:
                    break
                    
                poc_tag = item.find('span', class_='poc-tag poc-exists')
                if poc_tag:
                    link_tag = item.find('a')
                    if not link_tag:
                        continue
                        
                    title = link_tag.get_text(strip=True)
                    # æ¸…ç†æ ‡é¢˜ä¸­çš„æ—¥æœŸ (e.g., "2021-01-19æ³›å¾®...")
                    date_span = link_tag.find('span', class_='datetime')
                    if date_span:
                        date_text = date_span.get_text(strip=True)
                        title = title.replace(date_text, "").strip()
                        
                    href = link_tag['href']
                    full_url = base_url + href if href.startswith('/') else href
                    
                    valid_items.append({
                        "title": title,
                        "url": full_url
                    })

            if not valid_items:
                return f"æœªæ‰¾åˆ°å…³äº '{keyword}' ä¸”åŒ…å« POC ä»£ç çš„æ¼æ´æ¡ç›®ã€‚"

            # 3. å¹¶å‘è·å–è¯¦æƒ…é¡µ POC ä»£ç 
            for item in valid_items:
                tasks.append(_fetch_poc_detail(client, item["url"], item["title"]))
            
            # ç­‰å¾…æ‰€æœ‰è¯¦æƒ…é¡µæŠ“å–å®Œæˆ
            details = await asyncio.gather(*tasks)
            results.extend(details)

    except Exception as e:
        logger.error(f"POC æœç´¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return f"æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"

    # 4. æ ¼å¼åŒ–è¾“å‡º
    output = f"ğŸ” æœç´¢å…³é”®è¯: {keyword}\n"
    output += f"ğŸ¯ æ‰¾åˆ° {len(results)} ä¸ªåŒ…å« POC çš„æ¼æ´:\n\n"
    output += "\n".join(results)
    
    return output

async def _fetch_poc_detail(client: httpx.AsyncClient, url: str, title: str) -> str:
    """è¾…åŠ©å‡½æ•°ï¼šæŠ“å–å•ä¸ªé¡µé¢çš„ POC ä»£ç """
    try:
        resp = await client.get(url)
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        code_blocks = soup.find_all('code')
        poc_content = ""
        
        if code_blocks:
            for i, code in enumerate(code_blocks):
                text = code.get_text().strip()
                if text and text != "æš‚æ— ":
                    # æˆªæ–­è¿‡é•¿çš„ä»£ç ï¼Œé¿å… Token æº¢å‡ºï¼Œä½†ä¿ç•™è¶³å¤Ÿé•¿åº¦
                    if len(text) > 2000:
                        text = text[:2000] + "\n...(ä»£ç è¿‡é•¿å·²æˆªæ–­)..."
                    poc_content += f"\nğŸ’» POC ä»£ç ç‰‡æ®µ {i+1}:\n```\n{text}\n```\n"
        
        if not poc_content:
            poc_content = "\n(æœªæ£€æµ‹åˆ°æ ‡å‡†æ ¼å¼çš„ä»£ç å—ï¼Œè¯·è®¿é—®é“¾æ¥æŸ¥çœ‹)\n"

        return (
            f"ğŸ”´ **{title}**\n"
            f"ğŸ”— é“¾æ¥: {url}\n"
            f"{poc_content}"
            f"{'-'*40}"
        )
    except Exception as e:
        return f"ğŸ”´ **{title}**\nğŸ”— {url}\nâŒ è·å–è¯¦æƒ…å¤±è´¥: {str(e)}\n{'-'*40}"
